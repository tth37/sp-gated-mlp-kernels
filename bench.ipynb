{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class MockTransformerLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MockTransformerLayer, self).__init__()\n",
    "        self.fc = nn.Linear(1024, 1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class MockTransformer(nn.Module):\n",
    "    def __init__(self, n_layers):\n",
    "        super(MockTransformer, self).__init__()\n",
    "        self.layers = nn.ModuleList([MockTransformerLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "\n",
    "from torch.cuda import Stream\n",
    "\n",
    "\n",
    "s = Stream()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "t1_cpu_pinned = torch.randn(1024**2 * 5, pin_memory=True)\n",
    "t2_cpu_paged = torch.randn(1024**2 * 5, pin_memory=False)\n",
    "t3_cuda = torch.randn(1024**2 * 5, device=\"cuda:0\")\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\", torch.cuda.current_device())\n",
    "\n",
    "\n",
    "# The function we want to profile\n",
    "def inner(pinned: bool, streamed: bool):\n",
    "    with torch.cuda.stream(s) if streamed else contextlib.nullcontext():\n",
    "        if pinned:\n",
    "            t1_cuda = t1_cpu_pinned.to(device, non_blocking=True)\n",
    "        else:\n",
    "            t2_cuda = t2_cpu_paged.to(device, non_blocking=True)\n",
    "        t_star_cuda_h2d_event = s.record_event()\n",
    "    # This operation can be executed during the CPU to GPU copy if and only if the tensor is pinned and the copy is\n",
    "    #  done in the other stream\n",
    "    t3_cuda_mul = t3_cuda * t3_cuda * t3_cuda\n",
    "    t3_cuda_h2d_event = torch.cuda.current_stream().record_event()\n",
    "    t_star_cuda_h2d_event.synchronize()\n",
    "    t3_cuda_h2d_event.synchronize()\n",
    "\n",
    "\n",
    "# Our profiler: profiles the `inner` function and stores the results in a .json file\n",
    "def benchmark_with_profiler(\n",
    "    pinned,\n",
    "    streamed,\n",
    ") -> None:\n",
    "    torch._C._profiler._set_cuda_sync_enabled_val(True)\n",
    "    wait, warmup, active = 1, 1, 2\n",
    "    num_steps = wait + warmup + active\n",
    "    rank = 0\n",
    "    with torch.profiler.profile(\n",
    "        activities=[\n",
    "            torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA,\n",
    "        ],\n",
    "        schedule=torch.profiler.schedule(\n",
    "            wait=wait, warmup=warmup, active=active, repeat=1, skip_first=1\n",
    "        ),\n",
    "        with_stack=True,\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "    ) as prof:\n",
    "        for step_idx in range(1, num_steps + 1):\n",
    "            inner(streamed=streamed, pinned=pinned)\n",
    "            if rank is None or rank == 0:\n",
    "                prof.step()\n",
    "    prof.export_chrome_trace(f\"trace_streamed{int(streamed)}_pinned{int(pinned)}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_with_profiler(fn) -> None:\n",
    "    torch._C._profiler._set_cuda_sync_enabled_val(True)\n",
    "    wait, warmup, active = 1, 1, 2\n",
    "    num_steps = wait + warmup + active\n",
    "    with torch.profiler.profile(\n",
    "        activities=[\n",
    "            torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA,\n",
    "        ],\n",
    "        schedule=torch.profiler.schedule(\n",
    "            wait=wait, warmup=warmup, active=active, repeat=1, skip_first=1\n",
    "        ),\n",
    "    ) as prof:\n",
    "        for step_idx in range(1, num_steps + 1):\n",
    "            fn()\n",
    "            prof.step()\n",
    "    prof.export_chrome_trace(f\"trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Tuple\n",
    "from k_means_constrained import KMeansConstrained\n",
    "\n",
    "executor = ThreadPoolExecutor(max_workers=1)\n",
    "\n",
    "def reorder_random(act: np.ndarray, slice_size: int = 32) -> np.ndarray:\n",
    "    n_tokens = act.shape[0]\n",
    "    perm = np.random.permutation(n_tokens)\n",
    "    return perm\n",
    "\n",
    "def reorder_smat(act: torch.Tensor, slice_size: int, threshold: float = 0.7) -> torch.Tensor:\n",
    "    clusters = _smat(act, slice_size, threshold)\n",
    "    \n",
    "    perm = torch.zeros(act.shape[0], dtype=torch.int64)\n",
    "    current_index = 0\n",
    "    for cluster in clusters:\n",
    "        for token_index, _ in cluster:\n",
    "            perm[current_index] = token_index\n",
    "            current_index += 1\n",
    "\n",
    "    return perm\n",
    "\n",
    "def _smat(act: torch.Tensor, slice_size: int, threshold: float) -> List[List[Tuple[int, torch.Tensor]]]:\n",
    "    V = list(enumerate(act))\n",
    "    C = []\n",
    "    \n",
    "    def dist(v1: torch.Tensor, v2: torch.Tensor) -> float:\n",
    "        intersection = torch.sum(v1 & v2).item()\n",
    "        union = torch.sum(v1 | v2).item()\n",
    "        epsilon = 1e-8\n",
    "        return 1.0 - (intersection + epsilon) / (union + epsilon)\n",
    "    \n",
    "    while V:\n",
    "        v = V.pop(0)\n",
    "        c = [v]\n",
    "        pc = v[1]\n",
    "        \n",
    "        for w in V[:]:\n",
    "            if dist(w[1], pc) <= threshold:\n",
    "                V.remove(w)\n",
    "                c.append(w)\n",
    "                pc = pc | w[1]\n",
    "        C.append(c)\n",
    "    \n",
    "    return C\n",
    "\n",
    "stream = torch.cuda.Stream()\n",
    "\n",
    "def cluster(device_tensor):\n",
    "    host_tensor = torch.empty_like(device_tensor, device='cpu', pin_memory=True)\n",
    "    with torch.cuda.stream(stream):\n",
    "        host_tensor.copy_(device_tensor, non_blocking=True)\n",
    "        device_to_tensor_event = stream.record_event()\n",
    "    device_to_tensor_event.synchronize()\n",
    "    # host_ndarray = host_tensor.numpy()\n",
    "    # perm_ndarray = reorder_random(host_ndarray)\n",
    "    host_tensor_bool = (host_tensor > 0.01).to(torch.int8)\n",
    "    host_perm = reorder_smat(host_tensor_bool, 32)\n",
    "    device_perm = torch.empty_like(host_perm, device='cuda')\n",
    "    with torch.cuda.stream(stream):\n",
    "        device_perm.copy_(host_perm, non_blocking=True)\n",
    "        tensor_to_device_event = stream.record_event()\n",
    "    tensor_to_device_event.synchronize()\n",
    "    return device_perm\n",
    "\n",
    "class MockTransformerLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MockTransformerLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 14336, bias=False, dtype=torch.float16)\n",
    "        self.fc2 = nn.Linear(14336, 4096, bias=False, dtype=torch.float16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        future = executor.submit(cluster, x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        perm = future.result()\n",
    "        # print('perm:', perm)\n",
    "        return x\n",
    "\n",
    "model = MockTransformerLayer().cuda()\n",
    "x = torch.randn(512, 4096, dtype=torch.float16, device='cuda')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def fn():\n",
    "    model(x)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "benchmark_with_profiler(fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for clf.fit_predict(arr): 1.752438 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from k_means_constrained import KMeansConstrained\n",
    "from torch.utils.benchmark import Timer\n",
    "\n",
    "# Setup your data and model\n",
    "arr = np.random.rand(512, 4096)\n",
    "# convert arr to bool\n",
    "# arr = arr > 0.5\n",
    "clf = KMeansConstrained(\n",
    "    n_clusters=16,\n",
    "    size_min=32,\n",
    "    size_max=32,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Create a Timer object\n",
    "timer = Timer(\n",
    "    stmt='clf.fit_predict(arr)',\n",
    "    globals={'clf': clf, 'arr': arr}\n",
    ")\n",
    "\n",
    "# Run the timer\n",
    "result = timer.timeit(number=1)\n",
    "\n",
    "# Print the timing result\n",
    "print(f\"Time taken for clf.fit_predict(arr): {result.mean:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
