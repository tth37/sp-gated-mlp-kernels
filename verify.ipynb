{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sparsemm_kernels.up_dejavu import sparsemm_up_dejavu\n",
    "from sparsemm_kernels.up_dense import sparsemm_up_dense\n",
    "from sparsemm_kernels.up_neo import sparsemm_up_neo\n",
    "from sparsemm_kernels.up_torchsparse import sparsemm_up_torchsparse\n",
    "from sparsemm_kernels.up_cats import sparsemm_up_cats\n",
    "from sparsemm_kernels.utils import idx_to_mask\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EMBED_DIM = 5120\n",
    "HIDDEN_DIM = 13824\n",
    "P = 1\n",
    "Q = 10000\n",
    "\n",
    "X = torch.empty((BATCH_SIZE, EMBED_DIM), device=\"cuda\", dtype=torch.float16)\n",
    "Wup = torch.empty((HIDDEN_DIM, EMBED_DIM), device=\"cuda\", dtype=torch.float16)\n",
    "Wgate = torch.empty((HIDDEN_DIM, EMBED_DIM), device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "IDX = torch.randint(0, HIDDEN_DIM, (P, Q), device=\"cuda\", dtype=torch.int32)\n",
    "IDX = torch.sort(IDX, dim=1)[0]\n",
    "MASK = idx_to_mask(IDX, Q, HIDDEN_DIM)\n",
    "\n",
    "torch.nn.init.xavier_uniform_(X)\n",
    "torch.nn.init.xavier_uniform_(Wup)\n",
    "torch.nn.init.xavier_uniform_(Wgate)\n",
    "\n",
    "H_torchsparse = sparsemm_up_torchsparse(X, Wup, Wgate, IDX)\n",
    "H_dejavu = sparsemm_up_dejavu(\n",
    "    X, Wup, Wgate, IDX,\n",
    "    ACT_TYPE=\"fatrelu\", tune=True,\n",
    "    BLOCK_SIZE_M=16, BLOCK_SIZE_K=16, BLOCK_SIZE_Q=16,\n",
    "    num_stages=4, num_warps=4,\n",
    ")\n",
    "H_neo = sparsemm_up_neo(\n",
    "    X, Wup, Wgate, IDX,\n",
    "    ACT_TYPE=\"fatrelu\", tune=True,\n",
    "    BLOCK_SIZE_M=16, BLOCK_SIZE_K=16, BLOCK_SIZE_Q=16, GROUP_SIZE_Q=1,\n",
    "    num_stages=4, num_warps=4,\n",
    ")\n",
    "H_cats = sparsemm_up_cats(\n",
    "    X, Wup, Wgate, MASK,\n",
    "    ACT_TYPE=\"fatrelu\", tune=True,\n",
    "    BLOCK_SIZE_M=16, BLOCK_SIZE_N=16, BLOCK_SIZE_K=16, GROUP_SIZE_N=1,\n",
    "    num_stages=4, num_warps=4,\n",
    ")\n",
    "H_zero = torch.zeros_like(H_torchsparse)\n",
    "\n",
    "Wup_masked = Wup * MASK.reshape(-1, 1)\n",
    "Wgate_masked = Wgate * MASK.reshape(-1, 1)\n",
    "\n",
    "H_torchcats = sparsemm_up_dense(X, Wup_masked, Wgate_masked)\n",
    "\n",
    "print(torch.allclose(H_torchsparse, H_dejavu, atol=1e-3, rtol=1e-3))\n",
    "print(torch.allclose(H_torchsparse, H_neo, atol=1e-3, rtol=1e-3))\n",
    "print(torch.allclose(H_torchsparse, H_zero, atol=1e-3, rtol=1e-3))\n",
    "\n",
    "print(torch.allclose(H_torchcats, H_cats, atol=1e-3, rtol=1e-3))\n",
    "# # print(H_torchsparse)\n",
    "# # print(H_dejavu)\n",
    "# print(torch.max(torch.abs(H_torchsparse - H_dejavu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from sparsemm_kernels.down_dejavu import sparsemm_down_dejavu\n",
    "from sparsemm_kernels.down_splitk import sparsemm_down_splitk\n",
    "from sparsemm_kernels.down_torchsparse import sparsemm_down_torchsparse\n",
    "\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EMBED_DIM = 5120\n",
    "HIDDEN_DIM = 13824\n",
    "P = 1\n",
    "Q = 10000\n",
    "\n",
    "H = torch.empty((BATCH_SIZE, Q), device=\"cuda\", dtype=torch.float16)\n",
    "Wdown = torch.empty((HIDDEN_DIM, EMBED_DIM), device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "IDX = torch.randint(0, EMBED_DIM, (P, Q), device=\"cuda\", dtype=torch.int32)\n",
    "IDX = torch.sort(IDX, dim=1)[0]\n",
    "MASK = idx_to_mask(IDX, Q, EMBED_DIM)\n",
    "\n",
    "torch.nn.init.xavier_uniform_(H)\n",
    "torch.nn.init.xavier_uniform_(Wdown)\n",
    "\n",
    "H_torchsparse = sparsemm_down_torchsparse(H, Wdown, IDX)\n",
    "H_dejavu = sparsemm_down_dejavu(\n",
    "    H, Wdown, IDX, tune=True,\n",
    "    BLOCK_SIZE_M=16, BLOCK_SIZE_N=16, BLOCK_SIZE_Q=16,\n",
    "    num_stages=4, num_warps=4,\n",
    ")\n",
    "H_splitk = sparsemm_down_splitk(\n",
    "    H.T, Wdown, IDX, tune=True,\n",
    "    BLOCK_SIZE_M=16, BLOCK_SIZE_Q=64, BLOCK_SIZE_N=32, GROUP_SIZE_Q=1,\n",
    "    num_stages=4, num_warps=4,\n",
    ").T\n",
    "H_zero = torch.zeros_like(H_torchsparse)\n",
    "\n",
    "print(torch.allclose(H_torchsparse, H_dejavu, atol=1e-3, rtol=1e-3))\n",
    "print(torch.allclose(H_torchsparse, H_splitk, atol=1e-3, rtol=1e-3))\n",
    "print(torch.allclose(H_torchsparse, H_zero, atol=1e-3, rtol=1e-3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
